{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现两层全连接神经网络\n",
    "--------------\n",
    "\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss。\n",
    "- ##  $h = W_1X$\n",
    "- ## $h_{relu} = max(0, h)$\n",
    "- ## $y_{pred} = W_2 h_{relu}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案一：\n",
    "\n",
    "## 用numpy实现两层神经网络\n",
    "\n",
    "这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。\n",
    "- forward pass\n",
    "- loss\n",
    "- backward pass\n",
    "\n",
    "numpy ndarray是一个普通的n维array。它不知道任何关于深度学习或者梯度(gradient)的知识，也不知道计算图(computation graph)，只是一种用来计算数学运算的数据结构。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33580346.856427416\n",
      "50 16878.00562656999\n",
      "100 1175.5761415406482\n",
      "150 144.92942737452177\n",
      "200 21.349479971363497\n",
      "250 3.464356232454947\n",
      "300 0.588885572480889\n",
      "350 0.10300981682523466\n",
      "400 0.018371116368468575\n",
      "450 0.0033215922591358625\n",
      "500 0.0006066239639763492\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    h = x.dot(w1) # N * H\n",
    "    h_relu = np.maximum(h, 0) # N * H\n",
    "    y_pred = h_relu.dot(w2) # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    \n",
    "    if it % 50 == 0:\n",
    "        print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案二：\n",
    "\n",
    "## PyTorch: Tensors 实现两层神经网络\n",
    "\n",
    "使用PyTorch tensors来创建前向神经网络，计算损失，以及反向传播。\n",
    "\n",
    "一个PyTorch Tensor很像一个numpy的ndarray。但是它和numpy ndarray最大的区别是，PyTorch Tensor可以在CPU或者GPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26006564.0\n",
      "50 11465.49609375\n",
      "100 415.876220703125\n",
      "150 26.682729721069336\n",
      "200 2.1816067695617676\n",
      "250 0.20476067066192627\n",
      "300 0.021072817966341972\n",
      "350 0.0025582911912351847\n",
      "400 0.0004926429246552289\n",
      "450 0.0001552953472128138\n",
      "500 6.920313171576709e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    h = x.mm(w1) # N * H\n",
    "    h_relu = h.clamp(min=0) # N * H\n",
    "    y_pred = h_relu.mm(w2) # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案三：\n",
    "\n",
    "## PyTorch: Tensors 和 Autograd 实现两层神经网络\n",
    "\n",
    "\n",
    "PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。\n",
    "\n",
    "一个PyTorch的Tensor表示计算图中的一个节点。如果``x``是一个Tensor并且``x.requires_grad=True``那么``x.grad``是另一个储存着``x``当前梯度(相对于一个scalar，常常是loss)的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32493268.0\n",
      "50 13109.89453125\n",
      "100 448.79290771484375\n",
      "150 25.290950775146484\n",
      "200 1.723570704460144\n",
      "250 0.12972937524318695\n",
      "300 0.010481716133654118\n",
      "350 0.0011320256162434816\n",
      "400 0.00023621311993338168\n",
      "450 8.190183143597096e-05\n",
      "500 3.955412466893904e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum() # computation graph\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案四：\n",
    "\n",
    "## PyTorch: Tensors 和 optim 实现两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29260268.0\n",
      "50 12504.0068359375\n",
      "100 487.453125\n",
      "150 31.144981384277344\n",
      "200 2.379106283187866\n",
      "250 0.2002507746219635\n",
      "300 0.01808660849928856\n",
      "350 0.0019787990022450686\n",
      "400 0.000380392128136009\n",
      "450 0.00012120555038563907\n",
      "500 5.5032327509252355e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD([w1, w2], lr=learning_rate)\n",
    "\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum() # computation graph\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "#     with torch.no_grad():\n",
    "#         w1 -= learning_rate * w1.grad\n",
    "#         w2 -= learning_rate * w2.grad\n",
    "#         w1.grad.zero_()\n",
    "#         w2.grad.zero_()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案五：\n",
    "\n",
    "## PyTorch: Tensors 和 nn.MSELoss 实现两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28800416.0\n",
      "50 9263.63671875\n",
      "100 274.2176208496094\n",
      "150 13.594708442687988\n",
      "200 0.8313031196594238\n",
      "250 0.05708610638976097\n",
      "300 0.004427996929734945\n",
      "350 0.0005627150530926883\n",
      "400 0.00014180631842464209\n",
      "450 5.7230809034081176e-05\n",
      "500 3.0386532671400346e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD([w1, w2], lr=learning_rate)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    # loss = (y_pred - y).pow(2).sum() \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案六：\n",
    "\n",
    "## PyTorch: nn 实现两层神经网络\n",
    "\n",
    "使用PyTorch中nn这个库来构建网络。\n",
    "用PyTorch autograd来构建计算图和计算gradients，\n",
    "然后PyTorch会帮我们自动计算gradient。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33754652.0\n",
      "50 15310.271484375\n",
      "100 593.506103515625\n",
      "150 35.19988250732422\n",
      "200 2.520732879638672\n",
      "250 0.19948479533195496\n",
      "300 0.016804296523332596\n",
      "350 0.0017208210192620754\n",
      "400 0.000318250065902248\n",
      "450 0.0001016105234157294\n",
      "500 4.570413148030639e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=True), # w_1 * x + b_1\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=True),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    \n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters(): # param (tensor, grad)\n",
    "            param -= learning_rate * param.grad\n",
    "#             param.grad.zero_()\n",
    "            \n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案七：\n",
    "\n",
    "## PyTorch: nn 和 Optim 实现两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33509748.0\n",
      "50 11850.4736328125\n",
      "100 348.3865661621094\n",
      "150 16.511716842651367\n",
      "200 0.9526950716972351\n",
      "250 0.061549607664346695\n",
      "300 0.004501718562096357\n",
      "350 0.000535203143954277\n",
      "400 0.0001302134623983875\n",
      "450 5.2256727940402925e-05\n",
      "500 2.8227381335454993e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False), # w_1 * x + b_1\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "# learning_rate = 1e-4\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(501):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案八：\n",
    "\n",
    "## PyTorch:  自定义 nn Modules 实现两层神经网络 (显式参数)\n",
    "\n",
    "可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更加复杂的模型，就需要定义nn.Module模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.4040756225585938\n",
      "50 0.20220546424388885\n",
      "100 0.016015464439988136\n",
      "150 0.0016130581498146057\n",
      "200 0.00014704203931614757\n",
      "250 1.135985348810209e-05\n",
      "300 7.175092946454242e-07\n",
      "350 3.4551110417169184e-08\n",
      "400 1.1245625541889126e-09\n",
      "450 2.5372844450477494e-11\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # define the model architecture\n",
    "        self.W1 = nn.Parameter(nn.init.xavier_normal_(torch.Tensor(D_in, H)))\n",
    "        self.W2 = nn.Parameter(nn.init.xavier_normal_(torch.Tensor(H, D_out)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = x.mm(self.W1).clamp(min=0).mm(self.W2)\n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "# loss_fn = nn.MSELoss(reduction='sum')\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案九：\n",
    "\n",
    "## PyTorch: 自定义 nn Modules 实现两层神经网络 (隐式参数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 678.007568359375\n",
      "50 208.61647033691406\n",
      "100 53.54425811767578\n",
      "150 9.193710327148438\n",
      "200 1.215930700302124\n",
      "250 0.1409573256969452\n",
      "300 0.013429676182568073\n",
      "350 0.0009565682266838849\n",
      "400 4.8972426156979054e-05\n",
      "450 1.740896436785988e-06\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # define the model architecture\n",
    "        self.linear1 = torch.nn.Linear(D_in, H, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out, bias=False)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear2(self.linear1(x).clamp(min=0)) \n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    if it % 50 == 0:\n",
    "        print(it, loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
