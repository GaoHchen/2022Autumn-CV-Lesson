{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "EPOCH = 25\n",
    "LR = 0.0005\n",
    "DOWNLOAD_MNIST = False\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=torchvision.transforms.ToTensor(),\n",
    "                                        download=DOWNLOAD_MNIST, )\n",
    "test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)\n",
    "\n",
    "print(train_data.train_data.shape)\n",
    "\n",
    "train_x = torch.unsqueeze(train_data.train_data, dim=1).type(torch.FloatTensor) / 255.\n",
    "train_y = train_data.train_labels\n",
    "print(train_x.shape)\n",
    "\n",
    "test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:] / 255.  # Tensor on GPU\n",
    "test_y = test_data.test_labels[:]\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.3090 | test accuracy: 0.097\n",
      "Epoch:  0 | train loss: 1.0963 | test accuracy: 0.652\n",
      "Epoch:  0 | train loss: 0.6590 | test accuracy: 0.759\n",
      "Epoch:  0 | train loss: 0.7938 | test accuracy: 0.766\n",
      "Epoch:  1 | train loss: 0.4127 | test accuracy: 0.780\n",
      "Epoch:  1 | train loss: 0.6787 | test accuracy: 0.781\n",
      "Epoch:  1 | train loss: 0.5146 | test accuracy: 0.785\n",
      "Epoch:  1 | train loss: 0.4006 | test accuracy: 0.787\n",
      "Epoch:  2 | train loss: 0.5713 | test accuracy: 0.786\n",
      "Epoch:  2 | train loss: 0.5296 | test accuracy: 0.786\n",
      "Epoch:  2 | train loss: 0.4961 | test accuracy: 0.790\n",
      "Epoch:  2 | train loss: 0.6094 | test accuracy: 0.788\n",
      "Epoch:  3 | train loss: 0.6353 | test accuracy: 0.789\n",
      "Epoch:  3 | train loss: 0.3990 | test accuracy: 0.788\n",
      "Epoch:  3 | train loss: 0.3992 | test accuracy: 0.786\n",
      "Epoch:  3 | train loss: 0.3982 | test accuracy: 0.790\n",
      "Epoch:  4 | train loss: 0.4220 | test accuracy: 0.790\n",
      "Epoch:  4 | train loss: 0.4916 | test accuracy: 0.789\n",
      "Epoch:  4 | train loss: 0.4436 | test accuracy: 0.790\n",
      "Epoch:  4 | train loss: 0.6228 | test accuracy: 0.791\n",
      "Epoch:  5 | train loss: 0.4453 | test accuracy: 0.789\n",
      "Epoch:  5 | train loss: 0.6024 | test accuracy: 0.790\n",
      "Epoch:  5 | train loss: 0.5731 | test accuracy: 0.790\n",
      "Epoch:  5 | train loss: 0.3536 | test accuracy: 0.788\n",
      "Epoch:  6 | train loss: 0.5465 | test accuracy: 0.791\n",
      "Epoch:  6 | train loss: 0.3346 | test accuracy: 0.790\n",
      "Epoch:  6 | train loss: 0.4486 | test accuracy: 0.791\n",
      "Epoch:  6 | train loss: 0.5084 | test accuracy: 0.792\n",
      "Epoch:  7 | train loss: 0.4923 | test accuracy: 0.791\n",
      "Epoch:  7 | train loss: 0.4694 | test accuracy: 0.791\n",
      "Epoch:  7 | train loss: 0.7659 | test accuracy: 0.789\n",
      "Epoch:  7 | train loss: 0.4650 | test accuracy: 0.791\n",
      "Epoch:  8 | train loss: 0.4894 | test accuracy: 0.791\n",
      "Epoch:  8 | train loss: 0.4265 | test accuracy: 0.791\n",
      "Epoch:  8 | train loss: 0.3939 | test accuracy: 0.792\n",
      "Epoch:  8 | train loss: 0.4534 | test accuracy: 0.792\n",
      "Epoch:  9 | train loss: 0.5088 | test accuracy: 0.791\n",
      "Epoch:  9 | train loss: 0.3060 | test accuracy: 0.792\n",
      "Epoch:  9 | train loss: 0.4159 | test accuracy: 0.792\n",
      "Epoch:  9 | train loss: 0.5047 | test accuracy: 0.791\n",
      "Epoch:  10 | train loss: 0.6641 | test accuracy: 0.789\n",
      "Epoch:  10 | train loss: 0.4632 | test accuracy: 0.792\n",
      "Epoch:  10 | train loss: 0.3232 | test accuracy: 0.792\n",
      "Epoch:  10 | train loss: 0.5105 | test accuracy: 0.785\n",
      "Epoch:  11 | train loss: 0.4634 | test accuracy: 0.790\n",
      "Epoch:  11 | train loss: 0.4861 | test accuracy: 0.791\n",
      "Epoch:  11 | train loss: 0.3265 | test accuracy: 0.791\n",
      "Epoch:  11 | train loss: 0.4175 | test accuracy: 0.791\n",
      "Epoch:  12 | train loss: 0.4617 | test accuracy: 0.790\n",
      "Epoch:  12 | train loss: 0.5989 | test accuracy: 0.790\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m loss \u001b[39m=\u001b[39m loss_func(output, b_y\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     57\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 58\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m batch_i \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mps/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/mps/lib/python3.8/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/mps/lib/python3.8/site-packages/torch/optim/adam.py:162\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    160\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    163\u001b[0m          grads,\n\u001b[1;32m    164\u001b[0m          exp_avgs,\n\u001b[1;32m    165\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    167\u001b[0m          state_steps,\n\u001b[1;32m    168\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    170\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    171\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    177\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    179\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/mps/lib/python3.8/site-packages/torch/optim/adam.py:220\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 220\u001b[0m func(params,\n\u001b[1;32m    221\u001b[0m      grads,\n\u001b[1;32m    222\u001b[0m      exp_avgs,\n\u001b[1;32m    223\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    224\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    225\u001b[0m      state_steps,\n\u001b[1;32m    226\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    227\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    228\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    229\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    230\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    231\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    232\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    233\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    234\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable)\n",
      "File \u001b[0;32m~/anaconda3/envs/mps/lib/python3.8/site-packages/torch/optim/adam.py:265\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda, \u001b[39m\"\u001b[39m\u001b[39mIf capturable=True, params and state_steps must be CUDA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    268\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Classifer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifer, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=1 , out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "                                nn.Conv2d(in_channels=16 , out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "                                nn.BatchNorm2d(16),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout2d(p=0.1),\n",
    "                                nn.MaxPool2d(kernel_size=2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels=16 , out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "                                nn.Conv2d(in_channels=32 , out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "                                nn.BatchNorm2d(32),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout2d(p=0.2),\n",
    "                                nn.MaxPool2d(kernel_size=2))  \n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(in_channels=32 , out_channels=64, kernel_size=5, stride=1, padding=2),\n",
    "                                nn.BatchNorm2d(64),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout2d(p=0.25))\n",
    "        self.fc = nn.Sequential(nn.Linear(64*7*7,100),\n",
    "                                nn.BatchNorm1d(100),\n",
    "                                nn.Dropout(0.2),\n",
    "                                nn.Linear(100,10),\n",
    "                                nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        x = self.conv3(self.conv2(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Classifer()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# loss_func = nn.MSELoss()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "data_size = 20000\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    random_indx = np.random.permutation(data_size)\n",
    "    for batch_i in range(data_size // batch_size):\n",
    "        indx = random_indx[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "\n",
    "        b_x = train_x[indx, :]\n",
    "        b_y = train_y[indx]\n",
    "\n",
    "        output = model(b_x.to(device))\n",
    "    \n",
    "        loss = loss_func(output, b_y.to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch_i % 50 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_output = model(test_x.to(device))\n",
    "                pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "                accuracy = torch.sum(pred_y == test_y.to(device)).type(torch.FloatTensor) / test_y.size(0)\n",
    "                print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.cpu().numpy(), '| test accuracy: %.3f' % accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1,  ..., 3, 9, 5]) prediction number\n",
      "tensor([7, 2, 1,  ..., 3, 9, 5]) real number\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = fc(test_x)\n",
    "pred_y = torch.max(test_output, 1)[1].data.squeeze()  # move the computation in GPU\n",
    "\n",
    "print(pred_y, 'prediction number')\n",
    "print(test_y, 'real number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.5184,  -8.5824,  -0.6187,   3.0813,  -6.2037,  -1.7433, -11.3742,\n",
       "           7.9952,  -2.0846,  -0.2455],\n",
       "        [ -0.2027,  -0.6667,   6.3977,   2.8499, -13.8127,   1.6088,   0.5629,\n",
       "         -11.7980,   0.1761,  -9.4924],\n",
       "        [ -5.6566,   4.7353,  -0.8758,  -1.2021,  -2.0522,  -2.4119,  -2.2544,\n",
       "          -0.0299,  -0.5935,  -2.6085],\n",
       "        [  8.5143,  -9.4927,  -0.9003,  -2.4009,  -8.2609,  -1.1546,  -0.1482,\n",
       "          -2.3034,  -3.1212,  -1.2488],\n",
       "        [ -2.3296,  -6.8946,  -1.5348,  -4.9243,   6.0338,  -3.9376,   0.1403,\n",
       "          -2.0302,  -0.9111,   1.8864],\n",
       "        [ -7.3118,   6.0550,  -1.9936,  -1.3987,  -2.1926,  -4.3265,  -4.9954,\n",
       "           1.3960,  -0.7662,  -2.0353],\n",
       "        [ -6.3345,  -7.6327,  -7.6763,  -2.3350,   6.2307,   0.1029,  -4.3228,\n",
       "          -1.4999,   2.7457,   1.2418],\n",
       "        [ -7.2737,  -4.2829,  -1.8956,  -1.2960,   1.3093,  -1.6075,  -4.8665,\n",
       "          -1.8817,  -1.3280,   5.1082],\n",
       "        [  0.2546,  -6.3597,   1.0990,  -8.9196,   1.5377,   0.0202,   6.2394,\n",
       "         -10.2025,  -0.9192,  -3.5485],\n",
       "        [ -4.6866, -10.8633,  -6.6723,  -4.4085,   2.8267,  -2.7637,  -6.2096,\n",
       "           1.7673,   0.1774,   6.4981]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mps')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d2a7cdaa3147cfe62b452d20213d10d6c9fced858ab295933fec9aa2167f343"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
